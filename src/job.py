import os
import logging
import uuid
import subprocess
import time
import socket
import copy
import sys
import traceback


JOB_STATUSSES = [
    "queued",
    "running",
    "aborted",
    "done",
]

class Job:
    """
    Represent a single build of a job. Instances of this object are created,
    run through the `run()` method and then discarded. They hold temporary
    information such as an environment, exit_status and stdout of the script,
    etc.

    A JSON-dumpable dict representation of this object can be obtained from the
    `to_dict()` method. A new Job instance can be created from the module-level
    `from_dict()` function. This can be used to temporary recreate Job
    instances from on-disk representations for inspecting and re-running the
    job.
    """
    # NOTE: If you change the function definition, you also have to change
    # `to_dict` and `from_dict`!
    def __init__(self, jobdef_name, cmd, body, env, mail_to, work_dir=None,
                 prev_id=None):
        self.jobdef_name = jobdef_name
        self.cmd = cmd
        self.body = body
        self.env = env
        self.mail_to = list(mail_to)
        self.work_dir = work_dir
        self.status = None
        self.exit_code = None
        self.output = u''
        self.time_start = None
        self.time_end = None
        self.id = uuid.uuid4().hex
        self.prev_id = prev_id

    def set_status(self, status):
        """
        Update the current Job status ('queued', 'running', etc). See
        JOB_STATUSSES for all available statusses.
        """
        assert status in JOB_STATUSSES
        self.status = status

    def set_prev_id(self, prev_id):
        """
        The the ID of the job that came before this job.
        """
        self.prev_id = prev_id

    def run(self):
        """
        Run this job. Record the status in properties on this Job object.

        Returns -1 if the build was aborted (no error, but should continue
        building), 0 on success and 1 on failure.
        """
        logging.info("Running '{}' with command '{}' in working dir '{}'".format(self, self.cmd, self.work_dir))

        self.time_start = time.time()
        try:
            p = subprocess.Popen(self.cmd,
                                 shell=True,
                                 cwd=self.work_dir,
                                 stdin=subprocess.PIPE,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.STDOUT,
                                 env=self.env)

            # Stream output to self.output
            while True:
                stdout = p.stdout.readline().decode('utf8')
                if not stdout:
                    break
                self.output += stdout

            p.poll()
            self.exit_code = p.returncode
        except Exception as err:
            logging.exception(err)
            self.exit_code = 127
            self.output = traceback.format_exc(err)

        self.time_end = time.time()

        if self.exit_code == 255:
            return -1
        elif self.exit_code == 0:
            return 0
        else:
            return 1

    def to_dict(self):
        """
        Return a JSON-dumpable dict with information about this job.
        """
        d = {
            'jobdef_name': self.jobdef_name,
            'cmd': self.cmd,
            'body': self.body,
            'env': self.env,
            'mail_to': self.mail_to,
            'work_dir': self.work_dir,
            'status': self.status,
            'exit_code': self.exit_code,
            'output': self.output,
            'time_start': self.time_start,
            'time_end': self.time_end,
            'prev_id': self.prev_id,
            'id': self.id,
        }
        return d

    def __repr__(self):
        return "{}(id = {})".format(self.jobdef_name, self.id)


def from_dict(d):
    """
    Return a Job() instance from a dictionary that was previously generated by
    `Job.to_dict()`.

    This is used to load job status info from disk and recreating Job instances
    so we can programmatically introspect and re-run them.
    """
    job = Job(d['jobdef_name'],
              d['cmd'],
              d['body'],
              d['env'],
              d['mail_to'],
              d['work_dir'])
    job.status = d['status']
    job.exit_code = d['exit_code']
    job.output = d['output']
    job.time_start = d['time_start']
    job.time_end = d['time_end']
    job.prev_id = d['prev_id']
    job.id = d['id']
    return job

def make_env(request, jobdef, providers):
    env = {}
    env['HOME'] = os.path.expanduser('~')  # If started from init, no HOME is set.

    if not jobdef.clean_env:
        # Copy the environment from parent process (jerrybuild)
        env.update(os.environ)
    else:
        # Only use some specific values from the parent process.
        for k in ("PATH", "HOME"):
            env[k] = os.environ[k]

    # Add the path to the Jerrybuild shellscript tools to the PATH
    bin_basedir = os.path.dirname(os.path.realpath(sys.argv[0]))
    tools_path = os.path.join(bin_basedir, 'tools')
    env["PATH"] = "{}:{}".format(env["PATH"], tools_path)

    if jobdef.pass_query:
        # Add URL query (?foo=bar&baz=quux) to the environment
        for k, v in request.query.items():
            key = 'REQ_QRY_{}'.format(k)
            env[key] = v

    # Add request headers to the environment
    for k, v in request.headers.items():
        env["HEADER_{}".format(k.upper())] = v

    # Call the provider for this job defnition. It may also modify the
    # environment
    provider = providers[jobdef.provider]
    request_env = provider.normalize(request, jobdef)
    if request_env is False:
        # Normalization method aborted the request.
        return
    env.update(request_env)

    return env
